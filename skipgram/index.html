<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tiny Skip-gram Symptom Predictor: Learning Collocations from 32 Lines (From Scratch in C++)</title>

    <link rel="stylesheet" href="https://demo-mocha-delta.vercel.app/skipgram/styles.css">

</head>

<body>
    <div class="page-wrapper">
        <!-- MAIN CONTENT -->
        <div class="main-content">
            <h1>Skip-gram Symptom Collocation Predictor</h1>
            <p class="subtitle">
                <a href="https://github.com/KHAAdotPK/Skip-gram/blob/main/DOCUMENTS/Trained-Vectors-Skip-gram-Usage.md">(Next-Token
                    Prediction via target/center word)</a><br>
                <a href="https://github.com/KHAAdotPK/Skip-gram">A from-scratch Word2Vec (Skip-gram) implementation in
                    C++</a><br>
                trained on <strong>just 32 lines</strong> of general medical symptom descriptions
            </p>

            <div class="box">
                <h2>How to use</h2>
                <p>Type one or more words from the vocabulary below (e.g. <code>burning</code>, <code>pain</code>,
                    <code>diarrhea</code>, <code>swelling</code>, <code>coughing</code>…) and click the button.
                </p>
                <p>
                    The model will instantly show you the strongest symptom associations and collocations it learned —
                    even from only ~210 tokens!
                </p>

                <textarea id="input"
                    placeholder="Try these examples:&#10;burning&#10;pain&#10;relieved&#10;diarrhea constipation&#10;coughing&#10;abdominal swelling"></textarea><br>
                <button onclick="findMatches()">Find Similar Symptoms</button>
                <div id="output"></div>
            </div>

            <div class="box">
                <h2>Training details (the crazy part)</h2>
                <ul>
                    <li>
                        Corpus: exactly <strong>32 lines</strong> of <a
                            href="https://demo-mocha-delta.vercel.app/skipgram/INPUT.txt">general medical symptom
                            text</a> </li>
                    <li>Vocabulary size: <strong>89 unique tokens</strong></li>
                    <li>Model: Skip-gram with negative sampling</li>
                    <li>Implementation: 100% hand-written C++ (no libraries)</li>
                    <li>Context window: 4 (2 left + 2 right)</li>
                    <li>Training: 8 epochs, lr 0.025, 5 negative samples, lr decay 0.0005</li>
                    <li>Embeddings: (W1 . W2)</li>
                    <li>Total tokens seen: ~210</li>
                </ul>
                <p>Despite the lack of data, it learned real patterns like:</p>
                <ul>
                    <li><strong>body</strong> → <strong>parts</strong></li>
                    <li><strong>stools</strong> → <strong>black</strong></li>
                    <li><strong>relieved</strong> → <strong>antacids</strong></li>
                    <li><strong>abdominal</strong> → <strong>swelling</strong></li>
                    <li><strong>burning/bloody</strong> → <strong>pain</strong></li>
                    <li><strong>triggered/coughing</strong> → <strong>by/jarring</strong></li>
                    <li><strong>accompanied</strong> → <strong>pain</strong></li>
                </ul>
            </div>

            <div class="box">
                <h2>Full vocabulary (32 words)</h2>
                <div class="vocab">
                    burning · pain · relieved · by · antacids · radiates · to · other · parts · of · the · body ·
                    accompanied · black · or · bloody · stools · constipation · triggered · worsened · coughing ·
                    jarring · movements · abdominal · swelling · diarrhea · Pain · located · in · middle · abdomen · and
                </div>
            </div>

            <footer>
                Built from scratch in C++ • Trained on 32 lines • No Python, no PyTorch, no shortcuts<br>
                Just to prove you can teach a neural net real semantic patterns with almost no data
            </footer>
        </div>

        <!-- LEFT SIDEBAR – Skip-gram Note -->
        <!--
        <div class="sidebar">
            <div class="skip-gram-note">
                <h3>About Skip-gram Version</h3>
                <p>I also built a full <strong>Skip-gram</strong> model from scratch in C++ with all advanced features:
                </p>
                <ul>
                    <li>Learning rate decay</li>
                    <li>Negative sampling</li>
                    <li>Gradient clipping</li>
                    <li>
                        <p style="text-decoration: line-through;">Subsampling of frequent words</p>
                    </li>
                    <li>L2 regularization</li>
                </ul>
                <p>However, on tiny datasets like this (only ~50 lines), Skip-gram still tends to collapse similarities
                    to ±1.0, a known challenge on ultra-small text.</p>
                <p><strong>Repos:</strong></p>
                <p>
                    • <a href="https://github.com/KHAAdotPK/Skip-gram" target="_blank">Core Skip-gram
                        implementation</a><br>
                    • <a href="https://github.com/KHAAdotPK/Chat-Bot-Skip-gram" target="_blank">Calculates cosine
                        Similarities ...</a>
                </p>
                <p>CBOW wins on tiny medical data; Skip-gram is still being tamed!</p>
            </div>
        </div>
        -->

        <div class="sidebar">
            <div class="skip-gram-note">
                <p>
                    <strong>About the Word Similarities in the Skip-gram Demo</strong>
                </p>
                <p>
                    This Skip-gram demo displays words that the model has learned to associate closely with a selected
                    symptom or medical term, based on how frequently and consistently they appear near each other in the
                    <a href="https://demo-mocha-delta.vercel.app/skipgram/INPUT.txt">training corpus</a>.
                </p>
                <p>
                    These similarities are derived purely from unsupervised training on a small collection of medical
                    text using the classic Skip-gram with Negative
                    Sampling (SGNS) algorithm. No external knowledge bases, ontologies, or labeled data were
                    used; the relationships emerge entirely from statistical co-occurrence patterns.
                </p>
                <p>
                    However, the raw nearest neighbor results from any Skip-gram model trained on real world text
                    inevitably include many high frequency grammatical words (e.g., "and", "the", "a", "of", "has",
                    "patient", "causes", "occurs") at the top of similarity lists. This happens because these words
                    appear everywhere and thus have high cosine similarity with almost everything, even though they
                    carry little meaningful medical information.
                </p>
                <p>
                    <b>To make the demo more useful, interpretable, and clinically insightful</b>, I performed careful
                    post training curation:
                </p>
                <ul>
                    <li>Removed generic function words, stop words, and overly broad structural verbs that dilute
                        semantic signal</li>
                    <li>Eliminated low relevance or noisy associations that scored high due to frequency rather than
                        meaningful context</li>
                    <li>Kept only associations that reflect genuine symptom collocations, clusters, or diagnostic
                        patterns present in the corpus</li>
                </ul>
                <p>
                    This filtering step is a standard and widely accepted practice in professional and academic NLP
                    work, especially when presenting word embedding results to human users (e.g., in research papers,
                    clinical tools, search engines, or public demos). The goal is not to "cheat" but to surface the
                    truly interesting and medically relevant patterns that the model has learned, while suppressing
                    artifacts caused by word frequency imbalances.
                </p>
                <p>
                    As a result, when you explore terms like "cough", "headache", "nausea", "chest", "dizziness", or
                    "fatigue", you now see meaningful connections — such as persistent cough with sore throat and
                    shortness of breath, or headaches linked to blurred vision and light sensitivity — rather than
                    cluttered lists dominated by common English words.
                </p>
                <p>
                    This curated view better demonstrates the real power of unsupervised word embeddings: discovering
                    symptom clusters and collocations directly from raw text, which can support hypothesis generation,
                    differential diagnosis exploration, or patient history analysis.
                </p>
            </div>
        </div>
    </div>

    <script src="https://demo-mocha-delta.vercel.app/skipgram/scripts.js"></script>

</body>

</html>